{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484994ff-6d5d-4839-8288-989344a201d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "\n",
    "# Token y dataset desde widgets y secrets\n",
    "token = dbutils.secrets.get(\"claves\", \"token_app\")\n",
    "dbutils.widgets.text(\"codigo_dataset\", \"\")\n",
    "codigo_dataset = dbutils.widgets.get(\"codigo_dataset\")\n",
    "\n",
    "# Cliente Socrata autenticado\n",
    "client = Socrata(\"www.datos.gov.co\", str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527057f0-ab31-4f30-af26-99ae36f3da05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29290c60-b93e-477a-a0e1-2fc79c94d7bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "limit = 100000       # N√∫mero de registros por p√°gina (ajustable)\n",
    "offset = 0           # Comienza en 0\n",
    "write_mode = \"overwrite\"  # Solo overwrite en la primera escritura\n",
    "total_registros = 0  # Contador acumulado\n",
    "\n",
    "# ‚úÖ Crear la sesi√≥n Spark UNA sola vez al inicio\n",
    "from pyspark.sql import SparkSession\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Nombre de la tabla de destino\n",
    "# ---------------------------------------------\n",
    "nombre_tabla = \"main.diplomado_datos.ids_contratos_procesos\"\n",
    "\n",
    "print(f'üöÄ Iniciando la extracci√≥n de datos desde el dataset: {codigo_dataset}')\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Bucle principal de paginaci√≥n\n",
    "# ---------------------------------------------\n",
    "while True:\n",
    "    print(f'\\nüì¶ Descargando registros desde offset: {offset}')\n",
    "\n",
    "    # Construir la consulta SoQL\n",
    "    query = f\"\"\"\n",
    "    SELECT numero_del_contrato, numero_de_proceso, nit_de_la_entidad, documento_proveedor\n",
    "    LIMIT {limit}\n",
    "    OFFSET {offset}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        results = client.get(codigo_dataset, query=query)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en la descarga desde offset {offset}: {e}\")\n",
    "        break\n",
    "\n",
    "    if not results:\n",
    "        print(\"‚úÖ No hay m√°s registros para descargar.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Convertir resultados a DataFrame de pandas\n",
    "        df_pd = pd.DataFrame.from_records(results)\n",
    "\n",
    "        if df_pd.empty:\n",
    "            print(\"üì≠ Bloque vac√≠o, terminando.\")\n",
    "            break\n",
    "\n",
    "        # Validar y reactivar SparkSession si fue cerrada por timeout\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # Crear DataFrame de Spark desde pandas\n",
    "        df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "        # Escribir el bloque a tabla Delta\n",
    "        df_spark.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(write_mode) \\\n",
    "            .saveAsTable(nombre_tabla)\n",
    "\n",
    "        print(f\"‚úî Guardados {len(df_pd)} registros.\")\n",
    "        total_registros += len(df_pd)\n",
    "        print(f\"üìä Total acumulado: {total_registros}\")\n",
    "\n",
    "        # Avanzar al siguiente bloque\n",
    "        offset += limit\n",
    "        write_mode = \"append\"  # Cambiar el modo luego del primer write\n",
    "\n",
    "        # Evitar throttling\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error procesando bloque en offset {offset}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüèÅ Proceso finalizado. Total registros descargados: {total_registros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469153f2-72d6-4d7a-9834-a4c6de2ef9ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Par√°metros de descarga\n",
    "limit = 100000  # recomendable por estabilidad\n",
    "offset = 0\n",
    "write_mode = \"overwrite\"\n",
    "total_registros_descargados = 0\n",
    "\n",
    "print(f'Iniciando la extracci√≥n de datos del dataset {codigo_dataset}')\n",
    "\n",
    "# Loop con paginaci√≥n\n",
    "while True:\n",
    "    print(f'‚û§ Descargando desde offset: {offset}')\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT numero_del_contrato, numero_de_proceso, nit_de_la_entidad, documento_proveedor\n",
    "    LIMIT {limit}\n",
    "    OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener datos\n",
    "    results = client.get(codigo_dataset, query=query)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚úî No hay m√°s registros para descargar.\")\n",
    "        break\n",
    "    \n",
    "    # Convertir a Pandas y luego a Spark\n",
    "    df_pd = pd.DataFrame.from_records(results)\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    \n",
    "    # Escribir a Delta\n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(write_mode) \\\n",
    "        .saveAsTable(\"main.diplomado_datos.ids_contratos_procesos\")\n",
    "\n",
    "    # Cambiar modo de escritura a append\n",
    "    write_mode = \"append\"\n",
    "    offset += limit\n",
    "    total_registros_descargados += len(results)\n",
    "    print(f'‚úî Total acumulado: {total_registros_descargados}')\n",
    "\n",
    "print(\"‚úÖ Descarga completa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f21260-520e-4f0f-8142-ec86af002b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    " \n",
    "#client = Socrata(\"www.datos.gov.co\", None)\n",
    " \n",
    "token = dbutils.secrets.get(\"claves\",\"token_app\")\n",
    "codigo_dataset = dbutils.widgets.get(\"codigo_dataset\")\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "client = Socrata(\"www.datos.gov.co\",\n",
    "                  str(token))\n",
    " \n",
    "query=\"\"\"\n",
    "Select numero_del_contrato,numero_de_proceso, nit_de_la_entidad, documento_proveedor\n",
    "limit 20000000\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "results = client.get(codigo_dataset, query=query)\n",
    " \n",
    " \n",
    "# COMMAND ----------\n",
    " \n",
    "results_df = spark.createDataFrame(results)\n",
    " \n",
    "print(results_df.schema)\n",
    " \n",
    "# COMMAND ----------\n",
    " \n",
    "results_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"main.diplomado_datos.ids_contratos_procesos\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "Sodapy==2.2.0",
     "Pandas==2.3.0"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_de_datos_api_socrata",
   "widgets": {
    "codigo_dataset": {
     "currentValue": "rpmr-utcd",
     "nuid": "fac5561e-8ceb-404d-84f5-362431076b96",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "codigo_dataset",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "codigo_dataset",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
